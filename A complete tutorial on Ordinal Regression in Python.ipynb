{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95820ee4",
   "metadata": {},
   "source": [
    "https://analyticsindiamag.com/a-complete-tutorial-on-ordinal-regression-in-python/\n",
    "\n",
    "# A complete tutorial on Ordinal Regression in Python\n",
    "\n",
    "+ В статистике и машинном обучении ordinal (порядковая) регрессия - это вариант регрессионных моделей, который обычно используется, когда данные имеют порядковую переменную. \n",
    "+ Порядковая переменная - это тип переменной, в которой значения являются категориальными, но расположены по порядку.\n",
    "\n",
    "\n",
    "Порядковую регрессию можно разделить на две категории:\n",
    "\n",
    "+ Модель упорядоченного логита (Ordered logit model:): Мы также можем назвать эту модель упорядоченной логистической моделью (ordered logistic model). Например, у нас есть отзывы о каком-либо продукте в опросе как о плохом, хорошем, приятном и отличном, и мы хотим проанализировать, насколько хорошо эти ответы могут быть предсказаны для следующего продукта. Если вопросы количественные, то мы можем использовать эту модель. Мы можем думать о ней как о расширении логистической регрессии, которая позволяет использовать более двух категорий ответов, расположенных в упорядоченном порядке.  \n",
    "\n",
    "+ Модель упорядоченного пробита (Ordered probit model): Эту модель можно рассматривать как вариант пробит-модели, но с порядковой зависимой переменной, в которой может быть более двух исходов. Порядковая зависимая переменная может быть определена как переменная, в которой значения имеют естественный порядок, например, плохой, хороший, хороший, отличный.\n",
    "\n",
    "Для проведения порядковой регрессии мы можем использовать обобщенную линейную модель (GLM). \n",
    "\n",
    "GLM позволяет подгонять к данным вектор коэффициентов и набор пороговых значений. \n",
    "\n",
    "Допустим, в наборе данных есть наблюдения, представленные векторами длины p от X1 до Xn, и против этих наблюдений есть ответы от Y1 до Yn, в ответах каждая переменная является порядковой переменной. \n",
    "\n",
    "Мы можем представить Y как неубывающий вектор и применить вектор коэффициентов длины p и набор пороговых значений. \n",
    "\n",
    "Набор пороговых значений отвечает за разбиение линии действительных чисел на сегменты, соответствующие уровням ответов, которые аналогичны количеству сегментов.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437a16ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  carat      cut color clarity  depth  table  price     x     y  \\\n",
       "0           1   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98   \n",
       "1           2   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84   \n",
       "2           3   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07   \n",
       "3           4   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23   \n",
       "4           5   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35   \n",
       "\n",
       "      z  \n",
       "0  2.43  \n",
       "1  2.31  \n",
       "2  2.31  \n",
       "3  2.63  \n",
       "4  2.75  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file = 'data singles/diamonds.csv'\n",
    "data_diam = pd.read_csv(file)\n",
    "data_diam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4a7d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ideal        21551\n",
       "Premium      13791\n",
       "Very Good    12082\n",
       "Good          4906\n",
       "Fair          1610\n",
       "Name: cut, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# категорийная переменная cut \n",
    "data_diam['cut'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7eefe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      int64\n",
       "carat         float64\n",
       "cut            object\n",
       "color          object\n",
       "clarity        object\n",
       "depth         float64\n",
       "table         float64\n",
       "price           int64\n",
       "x             float64\n",
       "y             float64\n",
       "z             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_diam.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d48e83",
   "metadata": {},
   "source": [
    "## Data preprocessing \n",
    "\n",
    "Здесь мы видим, что у нас есть три переменные типа object, и в этой статье мы имеем дело с переменной cut. \n",
    "\n",
    "Для работы с порядковыми моделями из statsmodel нам необходимо преобразовать эту целевую переменную в категориальную упорядоченную форму, что можно сделать с помощью следующих строк кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97daad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=['Fair', 'Good', 'Ideal', 'Very Good', 'Premium'], ordered=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "cat_type = CategoricalDtype(categories=['Fair', 'Good', 'Ideal', 'Very Good', 'Premium'], ordered=True)\n",
    "data_diam['cut'] = data_diam['cut'].astype(cat_type)\n",
    "data_diam['cut'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4d7d9",
   "metadata": {},
   "source": [
    "+ Теперь в данных у нас есть переменные X, Y и Z, которые представляют собой высоту, ширину и глубину алмаза. \n",
    "+ Перемножив их, мы можем вычислить объем бриллианта. Давайте вычислим объем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27583374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>38.202030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>34.505856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>38.076885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>46.724580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>51.917250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  carat      cut color clarity  depth  table  price     volume\n",
       "0           1   0.23    Ideal     E     SI2   61.5   55.0    326  38.202030\n",
       "1           2   0.21  Premium     E     SI1   59.8   61.0    326  34.505856\n",
       "2           3   0.23     Good     E     VS1   56.9   65.0    327  38.076885\n",
       "3           4   0.29  Premium     I     VS2   62.4   58.0    334  46.724580\n",
       "4           5   0.31     Good     J     SI2   63.3   58.0    335  51.917250"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_diam['volume'] = data_diam['x'] * data_diam['y'] * data_diam['z']\n",
    "data_diam.drop(['x', 'y', 'z'], axis=1, inplace=True)\n",
    "data_diam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b21dfe59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=[20,12])\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.hist(data_diam['carat'], bins=20, color='b')\n",
    "plt.xlabel('Weight')\n",
    "plt.title('Distribution by Weight')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(data_diam['depth'], bins=20, color='r')\n",
    "plt.xlabel('Depth')\n",
    "plt.title('Distribution by Depth')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(data_diam['price'], bins=20, color='g')\n",
    "plt.xlabel('Price')\n",
    "plt.title('Distribution by Price')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(data_diam['volume'], bins=20, color='m')\n",
    "plt.xlabel('Volume')\n",
    "plt.title('Distribution by Volume')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc51b3",
   "metadata": {},
   "source": [
    "## Fitting Ordinal Regression\n",
    "### Ordered probit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3077df5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Aug 22 20:24:42 2015\n",
    "\n",
    "Author: Josef Perktold\n",
    "License: BSD-3\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "from statsmodels.compat.pandas import Appender\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy import stats\n",
    "\n",
    "from statsmodels.base.model import (\n",
    "    Model,\n",
    "    LikelihoodModel,\n",
    "    GenericLikelihoodModel,\n",
    "    GenericLikelihoodModelResults,\n",
    ")\n",
    "import statsmodels.base.wrapper as wrap\n",
    "# for results wrapper:\n",
    "import statsmodels.regression.linear_model as lm\n",
    "from statsmodels.tools.decorators import cache_readonly\n",
    "\n",
    "\n",
    "class OrderedModel(GenericLikelihoodModel):\n",
    "    \"\"\"Ordinal Model based on logistic or normal distribution\n",
    "\n",
    "    The parameterization corresponds to the proportional odds model in the\n",
    "    logistic case.\n",
    "    The model assumes that the endogenous variable is ordered but that the\n",
    "    labels have no numeric interpretation besides the ordering.\n",
    "\n",
    "    The model is based on a latent linear variable, where we observe only a\n",
    "    discretization.\n",
    "\n",
    "    y_latent = X beta + u\n",
    "\n",
    "    The observed variable is defined by the interval\n",
    "\n",
    "    y = {0 if y_latent <= cut_0\n",
    "         1 of cut_0 < y_latent <= cut_1\n",
    "         ...\n",
    "         K if cut_K < y_latent\n",
    "\n",
    "    The probability of observing y=k conditional on the explanatory variables\n",
    "    X is given by\n",
    "\n",
    "    prob(y = k | x) = Prob(cut_k < y_latent <= cut_k+1)\n",
    "                    = Prob(cut_k - x beta < u <= cut_k+1 - x beta\n",
    "                    = F(cut_k+1 - x beta) - F(cut_k - x beta)\n",
    "\n",
    "    Where F is the cumulative distribution of u which is either the normal\n",
    "    or the logistic distribution, but can be set to any other continuous\n",
    "    distribution. We use standardized distributions to avoid identifiability\n",
    "    problems.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    endog : array_like\n",
    "        Endogenous or dependent ordered categorical variable with k levels.\n",
    "        Labels or values of endog will internally transformed to consecutive\n",
    "        integers, 0, 1, 2, ...\n",
    "        pd.Series with ordered Categorical as dtype should be preferred as it\n",
    "        gives the order relation between the levels.\n",
    "        If endog is not a pandas Categorical, then categories are\n",
    "        sorted in lexicographic order (by numpy.unique).\n",
    "    exog : array_like\n",
    "        Exogenous, explanatory variables. This should not include an intercept.\n",
    "        pd.DataFrame are also accepted.\n",
    "        see Notes about constant when using formulas\n",
    "    offset : array_like\n",
    "        Offset is added to the linear prediction with coefficient equal to 1.\n",
    "    distr : string 'probit' or 'logit', or a distribution instance\n",
    "        The default is currently 'probit' which uses the normal distribution\n",
    "        and corresponds to an ordered Probit model. The distribution is\n",
    "        assumed to have the main methods of scipy.stats distributions, mainly\n",
    "        cdf, pdf and ppf. The inverse cdf, ppf, is only use to calculate\n",
    "        starting values.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Status: experimental, core results are verified, still subclasses\n",
    "    `GenericLikelihoodModel` which will change in future versions.\n",
    "\n",
    "    The parameterization of OrderedModel requires that there is no constant in\n",
    "    the model, neither explicit nor implicit. The constant is equivalent to\n",
    "    shifting all thresholds and is therefore not separately identified.\n",
    "\n",
    "    Patsy's formula specification does not allow a design matrix without\n",
    "    explicit or implicit constant if there are categorical variables (or maybe\n",
    "    splines) among explanatory variables. As workaround, statsmodels removes an\n",
    "    explicit intercept.\n",
    "\n",
    "    Consequently, there are two valid cases to get a design matrix without\n",
    "    intercept when using formulas:\n",
    "\n",
    "    - specify a model without explicit and implicit intercept which is possible\n",
    "      if there are only numerical variables in the model.\n",
    "    - specify a model with an explicit intercept which statsmodels will remove.\n",
    "\n",
    "    Models with an implicit intercept will be overparameterized, the parameter\n",
    "    estimates will not be fully identified, cov_params will not be invertible\n",
    "    and standard errors might contain nans. The computed results will be\n",
    "    dominated by numerical imprecision coming mainly from convergence tolerance\n",
    "    and numerical derivatives.\n",
    "\n",
    "    The model will raise a ValueError if a remaining constant is detected.\n",
    "\n",
    "    \"\"\"\n",
    "    _formula_max_endog = np.inf\n",
    "\n",
    "    def __init__(self, endog, exog, offset=None, distr='probit', **kwds):\n",
    "\n",
    "        if distr == 'probit':\n",
    "            self.distr = stats.norm\n",
    "        elif distr == 'logit':\n",
    "            self.distr = stats.logistic\n",
    "        else:\n",
    "            self.distr = distr\n",
    "\n",
    "        if offset is not None:\n",
    "            offset = np.asarray(offset)\n",
    "\n",
    "        self.offset = offset\n",
    "\n",
    "        endog, labels, is_pandas = self._check_inputs(endog, exog)\n",
    "\n",
    "        super(OrderedModel, self).__init__(endog, exog, **kwds)\n",
    "        k_levels = None  # initialize\n",
    "        if not is_pandas:\n",
    "            if self.endog.ndim == 1:\n",
    "                unique, index = np.unique(self.endog, return_inverse=True)\n",
    "                self.endog = index\n",
    "                labels = unique\n",
    "                if np.isnan(labels).any():\n",
    "                    msg = (\"NaN in dependent variable detected. \"\n",
    "                           \"Missing values need to be removed.\")\n",
    "                    raise ValueError(msg)\n",
    "            elif self.endog.ndim == 2:\n",
    "                if not hasattr(self, \"design_info\"):\n",
    "                    raise ValueError(\"2-dim endog not supported\")\n",
    "                # this branch is currently only in support of from_formula\n",
    "                # we need to initialize k_levels correctly for df_resid\n",
    "                k_levels = self.endog.shape[1]\n",
    "                labels = []\n",
    "                # Note: Doing the following here would break from_formula\n",
    "                # self.endog = self.endog.argmax(1)\n",
    "\n",
    "        if self.k_constant > 0:\n",
    "            raise ValueError(\"There should not be a constant in the model\")\n",
    "\n",
    "        self._initialize_labels(labels, k_levels=k_levels)\n",
    "\n",
    "        # adjust df\n",
    "        self.k_extra = self.k_levels - 1\n",
    "        self.df_model = self.k_vars\n",
    "        self.df_resid = self.nobs - (self.k_vars + self.k_extra)\n",
    "\n",
    "        self.results_class = OrderedResults\n",
    "\n",
    "    def _check_inputs(self, endog, exog):\n",
    "        \"\"\"Handle endog that is pandas Categorical.\n",
    "\n",
    "        Checks if self.distrib is legal and provides Pandas ordered Categorical\n",
    "        support for endog.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        endog : array_like\n",
    "            Endogenous, dependent variable, 1-D.\n",
    "        exog : array_like\n",
    "            Exogenous, explanatory variables.\n",
    "            Currently not used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        endog : array_like or pandas Series\n",
    "            If the original endog is a pandas ordered Categorical Series,\n",
    "            then the returned endog are the ``codes``, i.e. integer\n",
    "            representation of ordere categorical variable\n",
    "        labels : None or list\n",
    "            If original endog is pandas ordered Categorical Series, then the\n",
    "            categories are returned. Otherwise ``labels`` is None.\n",
    "        is_pandas : bool\n",
    "            This is True if original endog is a pandas ordered Categorical\n",
    "            Series and False otherwise.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(self.distr, stats.rv_continuous):\n",
    "            msg = (\n",
    "                f\"{self.distr.name} is not a scipy.stats distribution.\"\n",
    "            )\n",
    "            warnings.warn(msg)\n",
    "\n",
    "        labels = None\n",
    "        is_pandas = False\n",
    "        if isinstance(endog, pd.Series):\n",
    "            if isinstance(endog.dtypes, CategoricalDtype):\n",
    "                if not endog.dtype.ordered:\n",
    "                    warnings.warn(\"the endog has ordered == False, \"\n",
    "                                  \"risk of capturing a wrong order for the \"\n",
    "                                  \"categories. ordered == True preferred.\",\n",
    "                                  Warning)\n",
    "\n",
    "                endog_name = endog.name\n",
    "                labels = endog.values.categories\n",
    "                endog = endog.cat.codes\n",
    "                if endog.min() == -1:  # means there is a missing value\n",
    "                    raise ValueError(\"missing values in categorical endog are \"\n",
    "                                     \"not supported\")\n",
    "                endog.name = endog_name\n",
    "                is_pandas = True\n",
    "\n",
    "        return endog, labels, is_pandas\n",
    "\n",
    "    def _initialize_labels(self, labels, k_levels=None):\n",
    "        self.labels = labels\n",
    "        if k_levels is None:\n",
    "            self.k_levels = len(labels)\n",
    "        else:\n",
    "            self.k_levels = k_levels\n",
    "\n",
    "        if self.exog is not None:\n",
    "            self.nobs, self.k_vars = self.exog.shape\n",
    "        else:  # no exog in model\n",
    "            self.nobs, self.k_vars = self.endog.shape[0], 0\n",
    "\n",
    "        threshold_names = [str(x) + '/' + str(y)\n",
    "                           for x, y in zip(labels[:-1], labels[1:])]\n",
    "\n",
    "        # from GenericLikelihoodModel.fit\n",
    "        if self.exog is not None:\n",
    "            # avoid extending several times\n",
    "            if len(self.exog_names) > self.k_vars:\n",
    "                raise RuntimeError(\"something wrong with exog_names, too long\")\n",
    "            self.exog_names.extend(threshold_names)\n",
    "        else:\n",
    "            self.data.xnames = threshold_names\n",
    "\n",
    "    @classmethod\n",
    "    def from_formula(cls, formula, data, subset=None, drop_cols=None,\n",
    "                     *args, **kwargs):\n",
    "\n",
    "        # we want an explicit Intercept in the model that we can remove\n",
    "        # Removing constant with \"0 +\" or \"- 1\" does not work for categ. exog\n",
    "\n",
    "        endog_name = formula.split(\"~\")[0].strip()\n",
    "        original_endog = data[endog_name]\n",
    "\n",
    "        model = super(OrderedModel, cls).from_formula(\n",
    "            formula, data=data, drop_cols=[\"Intercept\"], *args, **kwargs)\n",
    "\n",
    "        if model.endog.ndim == 2:\n",
    "            if not (isinstance(original_endog.dtype, CategoricalDtype)\n",
    "                    and original_endog.dtype.ordered):\n",
    "                msg = (\"Only ordered pandas Categorical are supported as \"\n",
    "                       \"endog in formulas\")\n",
    "                raise ValueError(msg)\n",
    "\n",
    "            labels = original_endog.values.categories\n",
    "            model._initialize_labels(labels)\n",
    "            model.endog = model.endog.argmax(1)\n",
    "            model.data.ynames = endog_name\n",
    "\n",
    "        return model\n",
    "\n",
    "    from_formula.__func__.__doc__ = Model.from_formula.__doc__\n",
    "\n",
    "\n",
    "    def cdf(self, x):\n",
    "        \"\"\"Cdf evaluated at x.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like\n",
    "            Points at which cdf is evaluated. In the model `x` is the latent\n",
    "            variable plus threshold constants.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Value of the cumulative distribution function of the underlying latent\n",
    "        variable evaluated at x.\n",
    "        \"\"\"\n",
    "        return self.distr.cdf(x)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        \"\"\"Pdf evaluated at x\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like\n",
    "            Points at which cdf is evaluated. In the model `x` is the latent\n",
    "            variable plus threshold constants.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Value of the probability density function of the underlying latent\n",
    "        variable evaluated at x.\n",
    "        \"\"\"\n",
    "        return self.distr.pdf(x)\n",
    "\n",
    "    def prob(self, low, upp):\n",
    "        \"\"\"Interval probability.\n",
    "\n",
    "        Probability that value is in interval (low, upp], computed as\n",
    "\n",
    "            prob = cdf(upp) - cdf(low)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        low : array_like\n",
    "            lower bound for interval\n",
    "        upp : array_like\n",
    "            upper bound for interval\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or ndarray\n",
    "            Probability that value falls in interval (low, upp]\n",
    "\n",
    "        \"\"\"\n",
    "        return np.maximum(self.cdf(upp) - self.cdf(low), 0)\n",
    "\n",
    "    def transform_threshold_params(self, params):\n",
    "        \"\"\"transformation of the parameters in the optimization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : nd_array\n",
    "            Contains (exog_coef, transformed_thresholds) where exog_coef are\n",
    "            the coefficient for the explanatory variables in the linear term,\n",
    "            transformed threshold or cutoff points. The first, lowest threshold\n",
    "            is unchanged, all other thresholds are in terms of exponentiated\n",
    "            increments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        thresh : nd_array\n",
    "            Thresh are the thresholds or cutoff constants for the intervals.\n",
    "\n",
    "        \"\"\"\n",
    "        th_params = params[-(self.k_levels - 1):]\n",
    "        thresh = np.concatenate((th_params[:1],\n",
    "                                 np.exp(th_params[1:]))).cumsum()\n",
    "        thresh = np.concatenate(([-np.inf], thresh, [np.inf]))\n",
    "        return thresh\n",
    "\n",
    "    def transform_reverse_threshold_params(self, params):\n",
    "        \"\"\"obtain transformed thresholds from original thresholds or cutoffs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            Threshold values, cutoff constants for choice intervals, which\n",
    "            need to be monotonically increasing.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        thresh_params : ndarrray\n",
    "            Transformed threshold parameter.\n",
    "            The first, lowest threshold is unchanged, all other thresholds are\n",
    "            in terms of exponentiated increments.\n",
    "            Transformed parameters can be any real number without restrictions.\n",
    "\n",
    "        \"\"\"\n",
    "        thresh_params = np.concatenate((params[:1],\n",
    "                                        np.log(np.diff(params[:-1]))))\n",
    "        return thresh_params\n",
    "\n",
    "    def predict(self, params, exog=None, offset=None, which=\"prob\"):\n",
    "        \"\"\"\n",
    "        Predicted probabilities for each level of the ordinal endog.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            Parameters for the Model, (exog_coef, transformed_thresholds).\n",
    "        exog : array_like, optional\n",
    "            Design / exogenous data. If exog is None, model exog is used.\n",
    "        offset : array_like, optional\n",
    "            Offset is added to the linear prediction with coefficient\n",
    "            equal to 1. If offset is not provided and exog\n",
    "            is None, uses the model's offset if present.  If not, uses\n",
    "            0 as the default value.\n",
    "        which : {\"prob\", \"linpred\", \"cumprob\"}\n",
    "            Determines which statistic is predicted.\n",
    "\n",
    "            - prob : predicted probabilities to be in each choice. 2-dim.\n",
    "            - linear : 1-dim linear prediction of the latent variable\n",
    "              ``x b + offset``\n",
    "            - cumprob : predicted cumulative probability to be in choice k or\n",
    "              lower\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted values : ndarray\n",
    "            If which is \"prob\", then 2-dim predicted probabilities with\n",
    "            observations in rows and one column for each category or level of\n",
    "            the categorical dependent variable.\n",
    "            If which is \"cumprob\", then \"prob\" ar cumulatively added to get the\n",
    "            cdf at k, i.e. probability of observing choice k or lower.\n",
    "            If which is \"linpred\", then the conditional prediction of the\n",
    "            latent variable is returned. In this case, the return is\n",
    "            one-dimensional.\n",
    "        \"\"\"\n",
    "        # note, exog and offset handling is in linpred\n",
    "\n",
    "        thresh = self.transform_threshold_params(params)\n",
    "        xb = self._linpred(params, exog=exog, offset=offset)\n",
    "        if which == \"linpred\":\n",
    "            return xb\n",
    "        xb = xb[:, None]\n",
    "        low = thresh[:-1] - xb\n",
    "        upp = thresh[1:] - xb\n",
    "        if which == \"prob\":\n",
    "            prob = self.prob(low, upp)\n",
    "            return prob\n",
    "        elif which in [\"cum\", \"cumprob\"]:\n",
    "            cumprob = self.cdf(upp)\n",
    "            return cumprob\n",
    "        else:\n",
    "            raise ValueError(\"`which` is not available\")\n",
    "\n",
    "    def _linpred(self, params, exog=None, offset=None):\n",
    "        \"\"\"Linear prediction of latent variable `x b + offset`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            Parameters for the model, (exog_coef, transformed_thresholds)\n",
    "        exog : array_like, optional\n",
    "            Design / exogenous data. Is exog is None, model exog is used.\n",
    "        offset : array_like, optional\n",
    "            Offset is added to the linear prediction with coefficient\n",
    "            equal to 1. If offset is not provided and exog\n",
    "            is None, uses the model's offset if present.  If not, uses\n",
    "            0 as the default value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        linear : ndarray\n",
    "            1-dim linear prediction given by exog times linear params plus\n",
    "            offset. This is the prediction for the underlying latent variable.\n",
    "            If exog and offset are None, then the predicted values are zero.\n",
    "\n",
    "        \"\"\"\n",
    "        if exog is None:\n",
    "            exog = self.exog\n",
    "            if offset is None:\n",
    "                offset = self.offset\n",
    "        else:\n",
    "            if offset is None:\n",
    "                offset = 0\n",
    "\n",
    "        if offset is not None:\n",
    "            offset = np.asarray(offset)\n",
    "\n",
    "        if exog is not None:\n",
    "            _exog = np.asarray(exog)\n",
    "            _params = np.asarray(params)\n",
    "            linpred = _exog.dot(_params[:-(self.k_levels - 1)])\n",
    "        else:  # means self.exog is also None\n",
    "            linpred = np.zeros(self.nobs)\n",
    "        if offset is not None:\n",
    "            linpred += offset\n",
    "        return linpred\n",
    "\n",
    "    def _bounds(self, params):\n",
    "        \"\"\"Integration bounds for the observation specific interval.\n",
    "\n",
    "        This defines the lower and upper bounds for the intervals of the\n",
    "        choices of all observations.\n",
    "\n",
    "        The bounds for observation are given by\n",
    "\n",
    "            a_{k_i-1} - linpred_i, a_k_i - linpred_i\n",
    "\n",
    "        where\n",
    "        - k_i is the choice in observation i.\n",
    "        - a_{k_i-1} and a_k_i are thresholds (cutoffs) for choice k_i\n",
    "        - linpred_i is the linear prediction for observation i\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            Parameters for the model, (exog_coef, transformed_thresholds)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        low : ndarray\n",
    "            Lower bounds for choice intervals of each observation,\n",
    "            1-dim with length nobs\n",
    "        upp : ndarray\n",
    "            Upper bounds for choice intervals of each observation,\n",
    "            1-dim with length nobs.\n",
    "\n",
    "        \"\"\"\n",
    "        thresh = self.transform_threshold_params(params)\n",
    "\n",
    "        thresh_i_low = thresh[self.endog]\n",
    "        thresh_i_upp = thresh[self.endog + 1]\n",
    "        xb = self._linpred(params)\n",
    "        low = thresh_i_low - xb\n",
    "        upp = thresh_i_upp - xb\n",
    "        return low, upp\n",
    "\n",
    "    @Appender(GenericLikelihoodModel.loglike.__doc__)\n",
    "    def loglike(self, params):\n",
    "\n",
    "        return self.loglikeobs(params).sum()\n",
    "\n",
    "    def loglikeobs(self, params):\n",
    "        \"\"\"\n",
    "        Log-likelihood of OrderdModel for all observations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The parameters of the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loglike_obs : array_like\n",
    "            The log likelihood for each observation of the model evaluated\n",
    "            at ``params``.\n",
    "        \"\"\"\n",
    "        low, upp = self._bounds(params)\n",
    "        prob = self.prob(low, upp)\n",
    "        return np.log(prob + 1e-20)\n",
    "\n",
    "    def score_obs_(self, params):\n",
    "        \"\"\"score, first derivative of loglike for each observations\n",
    "\n",
    "        This currently only implements the derivative with respect to the\n",
    "        exog parameters, but not with respect to threshold parameters.\n",
    "\n",
    "        \"\"\"\n",
    "        low, upp = self._bounds(params)\n",
    "\n",
    "        prob = self.prob(low, upp)\n",
    "        pdf_upp = self.pdf(upp)\n",
    "        pdf_low = self.pdf(low)\n",
    "\n",
    "        # TODO the following doesn't work yet because of the incremental exp\n",
    "        # parameterization. The following was written based on Greene for the\n",
    "        # simple non-incremental parameterization.\n",
    "        # k = self.k_levels - 1\n",
    "        # idx = self.endog\n",
    "        # score_factor = np.zeros((self.nobs, k + 1 + 2)) #+2 avoids idx bounds\n",
    "        #\n",
    "        # rows = np.arange(self.nobs)\n",
    "        # shift = 1\n",
    "        # score_factor[rows, shift + idx-1] = -pdf_low\n",
    "        # score_factor[rows, shift + idx] = pdf_upp\n",
    "        # score_factor[:, 0] = pdf_upp - pdf_low\n",
    "        score_factor = (pdf_upp - pdf_low)[:, None]\n",
    "        score_factor /= prob[:, None]\n",
    "\n",
    "        so = np.column_stack((-score_factor[:, :1] * self.exog,\n",
    "                              score_factor[:, 1:]))\n",
    "        return so\n",
    "\n",
    "    @property\n",
    "    def start_params(self):\n",
    "        \"\"\"Start parameters for the optimization corresponding to null model.\n",
    "\n",
    "        The threshold are computed from the observed frequencies and\n",
    "        transformed to the exponential increments parameterization.\n",
    "        The parameters for explanatory variables are set to zero.\n",
    "        \"\"\"\n",
    "        # start params based on model without exog\n",
    "        freq = np.bincount(self.endog) / len(self.endog)\n",
    "        start_ppf = self.distr.ppf(np.clip(freq.cumsum(), 0, 1))\n",
    "        start_threshold = self.transform_reverse_threshold_params(start_ppf)\n",
    "        start_params = np.concatenate((np.zeros(self.k_vars), start_threshold))\n",
    "        return start_params\n",
    "\n",
    "    @Appender(LikelihoodModel.fit.__doc__)\n",
    "    def fit(self, start_params=None, method='nm', maxiter=500, full_output=1,\n",
    "            disp=1, callback=None, retall=0, **kwargs):\n",
    "\n",
    "        fit_method = super(OrderedModel, self).fit\n",
    "        mlefit = fit_method(start_params=start_params,\n",
    "                            method=method, maxiter=maxiter,\n",
    "                            full_output=full_output,\n",
    "                            disp=disp, callback=callback, **kwargs)\n",
    "        # use the proper result class\n",
    "        ordmlefit = OrderedResults(self, mlefit)\n",
    "\n",
    "        # TODO: temporary, needs better fix, modelwc adds 1 by default\n",
    "        ordmlefit.hasconst = 0\n",
    "\n",
    "        result = OrderedResultsWrapper(ordmlefit)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class OrderedResults(GenericLikelihoodModelResults):\n",
    "    \"\"\"Results class for OrderedModel\n",
    "\n",
    "    This class inherits from GenericLikelihoodModelResults and not all\n",
    "    inherited methods might be appropriate in this case.\n",
    "    \"\"\"\n",
    "\n",
    "    def pred_table(self):\n",
    "        \"\"\"prediction table\n",
    "\n",
    "        returns pandas DataFrame\n",
    "\n",
    "        \"\"\"\n",
    "        # todo: add category labels\n",
    "        categories = np.arange(self.model.k_levels)\n",
    "        observed = pd.Categorical(self.model.endog,\n",
    "                                  categories=categories, ordered=True)\n",
    "        predicted = pd.Categorical(self.predict().argmax(1),\n",
    "                                   categories=categories, ordered=True)\n",
    "        table = pd.crosstab(predicted,\n",
    "                            observed.astype(int),\n",
    "                            margins=True,\n",
    "                            dropna=False).T.fillna(0)\n",
    "        return table\n",
    "\n",
    "    @cache_readonly\n",
    "    def llnull(self):\n",
    "        \"\"\"\n",
    "        Value of the loglikelihood of model without explanatory variables\n",
    "        \"\"\"\n",
    "        params_null = self.model.start_params\n",
    "        return self.model.loglike(params_null)\n",
    "\n",
    "    # next 3 are copied from discrete\n",
    "    @cache_readonly\n",
    "    def prsquared(self):\n",
    "        \"\"\"\n",
    "        McFadden's pseudo-R-squared. `1 - (llf / llnull)`\n",
    "        \"\"\"\n",
    "        return 1 - self.llf/self.llnull\n",
    "\n",
    "    @cache_readonly\n",
    "    def llr(self):\n",
    "        \"\"\"\n",
    "        Likelihood ratio chi-squared statistic; `-2*(llnull - llf)`\n",
    "        \"\"\"\n",
    "        return -2*(self.llnull - self.llf)\n",
    "\n",
    "    @cache_readonly\n",
    "    def llr_pvalue(self):\n",
    "        \"\"\"\n",
    "        The chi-squared probability of getting a log-likelihood ratio\n",
    "        statistic greater than llr.  llr has a chi-squared distribution\n",
    "        with degrees of freedom `df_model`.\n",
    "        \"\"\"\n",
    "        # number of restrictions is number of exog\n",
    "        return stats.distributions.chi2.sf(self.llr, self.model.k_vars)\n",
    "\n",
    "    @cache_readonly\n",
    "    def resid_prob(self):\n",
    "        \"\"\"probability residual\n",
    "\n",
    "        Probability-scale residual is ``P(Y < y) − P(Y > y)`` where `Y` is the\n",
    "        observed choice and ``y`` is a random variable corresponding to the\n",
    "        predicted distribution.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        Shepherd BE, Li C, Liu Q (2016) Probability-scale residuals for\n",
    "        continuous, discrete, and censored data.\n",
    "        The Canadian Journal of Statistics. 44:463–476.\n",
    "\n",
    "        Li C and Shepherd BE (2012) A new residual for ordinal outcomes.\n",
    "        Biometrika. 99: 473–480\n",
    "\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.diagnostic_gen import prob_larger_ordinal_choice\n",
    "        endog = self.model.endog\n",
    "        fitted = self.predict()\n",
    "        r = prob_larger_ordinal_choice(fitted)[1]\n",
    "        resid_prob = r[np.arange(endog.shape[0]), endog]\n",
    "        return resid_prob\n",
    "\n",
    "\n",
    "class OrderedResultsWrapper(lm.RegressionResultsWrapper):\n",
    "    pass\n",
    "\n",
    "\n",
    "wrap.populate_wrapper(OrderedResultsWrapper, OrderedResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afcfee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_prob = OrderedModel(data_diam['cut'],\n",
    "                       data_diam[['volume', 'price', 'carat']], \n",
    "                       distr='probit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce40a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.369122\n",
      "         Iterations: 29\n",
      "         Function evaluations: 35\n",
      "         Gradient evaluations: 35\n"
     ]
    }
   ],
   "source": [
    "res_prob = mob_prob.fit(method='bfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0955b0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OrderedModel Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>cut</td>        <th>  Log-Likelihood:    </th> <td> -73850.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>OrderedModel</td>    <th>  AIC:               </th> <td>1.477e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>           <td>Maximum Likelihood</td> <th>  BIC:               </th> <td>1.477e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Tue, 19 Mar 2024</td>  <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>16:00:04</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td> 53940</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td> 53933</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>     3</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>volume</th>            <td>    0.0060</td> <td>    0.001</td> <td>   10.108</td> <td> 0.000</td> <td>    0.005</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>price</th>             <td> 3.511e-05</td> <td> 3.01e-06</td> <td>   11.658</td> <td> 0.000</td> <td> 2.92e-05</td> <td>  4.1e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carat</th>             <td>   -1.1049</td> <td>    0.097</td> <td>  -11.353</td> <td> 0.000</td> <td>   -1.296</td> <td>   -0.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fair/Good</th>         <td>   -1.8589</td> <td>    0.016</td> <td> -119.403</td> <td> 0.000</td> <td>   -1.889</td> <td>   -1.828</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Good/Ideal</th>        <td>   -0.3292</td> <td>    0.014</td> <td>  -24.056</td> <td> 0.000</td> <td>   -0.356</td> <td>   -0.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Ideal/Very Good</th>   <td>    0.2053</td> <td>    0.006</td> <td>   35.080</td> <td> 0.000</td> <td>    0.194</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Very Good/Premium</th> <td>   -0.4986</td> <td>    0.008</td> <td>  -60.605</td> <td> 0.000</td> <td>   -0.515</td> <td>   -0.483</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                             OrderedModel Results                             \n",
       "==============================================================================\n",
       "Dep. Variable:                    cut   Log-Likelihood:                -73850.\n",
       "Model:                   OrderedModel   AIC:                         1.477e+05\n",
       "Method:            Maximum Likelihood   BIC:                         1.477e+05\n",
       "Date:                Tue, 19 Mar 2024                                         \n",
       "Time:                        16:00:04                                         \n",
       "No. Observations:               53940                                         \n",
       "Df Residuals:                   53933                                         \n",
       "Df Model:                           3                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "volume                0.0060      0.001     10.108      0.000       0.005       0.007\n",
       "price              3.511e-05   3.01e-06     11.658      0.000    2.92e-05     4.1e-05\n",
       "carat                -1.1049      0.097    -11.353      0.000      -1.296      -0.914\n",
       "Fair/Good            -1.8589      0.016   -119.403      0.000      -1.889      -1.828\n",
       "Good/Ideal           -0.3292      0.014    -24.056      0.000      -0.356      -0.302\n",
       "Ideal/Very Good       0.2053      0.006     35.080      0.000       0.194       0.217\n",
       "Very Good/Premium    -0.4986      0.008    -60.605      0.000      -0.515      -0.483\n",
       "=====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_prob.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3ab23",
   "metadata": {},
   "source": [
    "### Ordered logit regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c73dcd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.369215\n",
      "         Iterations: 36\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OrderedModel Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>cut</td>        <th>  Log-Likelihood:    </th> <td> -73855.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>OrderedModel</td>    <th>  AIC:               </th> <td>1.477e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>           <td>Maximum Likelihood</td> <th>  BIC:               </th> <td>1.477e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Tue, 19 Mar 2024</td>  <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>16:02:32</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td> 53940</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td> 53933</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>     3</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>volume</th>            <td>    0.0104</td> <td>    0.002</td> <td>    5.875</td> <td> 0.000</td> <td>    0.007</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>price</th>             <td> 2.348e-05</td> <td> 5.31e-06</td> <td>    4.425</td> <td> 0.000</td> <td> 1.31e-05</td> <td> 3.39e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carat</th>             <td>   -1.5407</td> <td>    0.286</td> <td>   -5.394</td> <td> 0.000</td> <td>   -2.100</td> <td>   -0.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fair/Good</th>         <td>   -3.2836</td> <td>    0.031</td> <td> -105.758</td> <td> 0.000</td> <td>   -3.344</td> <td>   -3.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Good/Ideal</th>        <td>    0.4038</td> <td>    0.015</td> <td>   27.062</td> <td> 0.000</td> <td>    0.375</td> <td>    0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Ideal/Very Good</th>   <td>    0.7301</td> <td>    0.006</td> <td>  116.508</td> <td> 0.000</td> <td>    0.718</td> <td>    0.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Very Good/Premium</th> <td>   -0.0055</td> <td>    0.008</td> <td>   -0.653</td> <td> 0.514</td> <td>   -0.022</td> <td>    0.011</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                             OrderedModel Results                             \n",
       "==============================================================================\n",
       "Dep. Variable:                    cut   Log-Likelihood:                -73855.\n",
       "Model:                   OrderedModel   AIC:                         1.477e+05\n",
       "Method:            Maximum Likelihood   BIC:                         1.477e+05\n",
       "Date:                Tue, 19 Mar 2024                                         \n",
       "Time:                        16:02:32                                         \n",
       "No. Observations:               53940                                         \n",
       "Df Residuals:                   53933                                         \n",
       "Df Model:                           3                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "volume                0.0104      0.002      5.875      0.000       0.007       0.014\n",
       "price              2.348e-05   5.31e-06      4.425      0.000    1.31e-05    3.39e-05\n",
       "carat                -1.5407      0.286     -5.394      0.000      -2.100      -0.981\n",
       "Fair/Good            -3.2836      0.031   -105.758      0.000      -3.344      -3.223\n",
       "Good/Ideal            0.4038      0.015     27.062      0.000       0.375       0.433\n",
       "Ideal/Very Good       0.7301      0.006    116.508      0.000       0.718       0.742\n",
       "Very Good/Premium    -0.0055      0.008     -0.653      0.514      -0.022       0.011\n",
       "=====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mob_prob = OrderedModel(data_diam['cut'],\n",
    "                       data_diam[['volume', 'price', 'carat']], \n",
    "                       distr='logit')\n",
    "res_log = mob_prob.fit(method='bfgs')\n",
    "res_log.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cad0e02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fair</th>\n",
       "      <th>Good</th>\n",
       "      <th>Ideal</th>\n",
       "      <th>Very Good</th>\n",
       "      <th>Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034436</td>\n",
       "      <td>0.103082</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>0.214953</td>\n",
       "      <td>0.225510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034689</td>\n",
       "      <td>0.103730</td>\n",
       "      <td>0.422982</td>\n",
       "      <td>0.214408</td>\n",
       "      <td>0.224191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034479</td>\n",
       "      <td>0.103191</td>\n",
       "      <td>0.422181</td>\n",
       "      <td>0.214861</td>\n",
       "      <td>0.225288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034561</td>\n",
       "      <td>0.103403</td>\n",
       "      <td>0.422497</td>\n",
       "      <td>0.214683</td>\n",
       "      <td>0.224856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033797</td>\n",
       "      <td>0.101438</td>\n",
       "      <td>0.419519</td>\n",
       "      <td>0.216332</td>\n",
       "      <td>0.228914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.034772</td>\n",
       "      <td>0.103944</td>\n",
       "      <td>0.423299</td>\n",
       "      <td>0.214227</td>\n",
       "      <td>0.223758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.034725</td>\n",
       "      <td>0.103822</td>\n",
       "      <td>0.423119</td>\n",
       "      <td>0.214330</td>\n",
       "      <td>0.224005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.034542</td>\n",
       "      <td>0.103355</td>\n",
       "      <td>0.422425</td>\n",
       "      <td>0.214724</td>\n",
       "      <td>0.224954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.034529</td>\n",
       "      <td>0.103320</td>\n",
       "      <td>0.422374</td>\n",
       "      <td>0.214753</td>\n",
       "      <td>0.225024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.034249</td>\n",
       "      <td>0.102602</td>\n",
       "      <td>0.421295</td>\n",
       "      <td>0.215357</td>\n",
       "      <td>0.226497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Fair      Good     Ideal  Very Good   Premium\n",
       "0  0.034436  0.103082  0.422018   0.214953  0.225510\n",
       "1  0.034689  0.103730  0.422982   0.214408  0.224191\n",
       "2  0.034479  0.103191  0.422181   0.214861  0.225288\n",
       "3  0.034561  0.103403  0.422497   0.214683  0.224856\n",
       "4  0.033797  0.101438  0.419519   0.216332  0.228914\n",
       "5  0.034772  0.103944  0.423299   0.214227  0.223758\n",
       "6  0.034725  0.103822  0.423119   0.214330  0.224005\n",
       "7  0.034542  0.103355  0.422425   0.214724  0.224954\n",
       "8  0.034529  0.103320  0.422374   0.214753  0.225024\n",
       "9  0.034249  0.102602  0.421295   0.215357  0.226497"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can make the prediction from the model\n",
    "predicted = res_log.model.predict(res_log.params, \n",
    "                                 exog=data_diam[['volume', 'price', 'carat']])\n",
    "predicted = pd.DataFrame(predicted)\n",
    "predicted.columns = ['Fair', 'Good', 'Ideal', 'Very Good', 'Premium']\n",
    "predicted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "138f3495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>38.202030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>34.505856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>38.076885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>46.724580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>51.917250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>J</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>62.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>336</td>\n",
       "      <td>38.693952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>I</td>\n",
       "      <td>VVS1</td>\n",
       "      <td>62.3</td>\n",
       "      <td>57.0</td>\n",
       "      <td>336</td>\n",
       "      <td>38.830870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.26</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>61.9</td>\n",
       "      <td>55.0</td>\n",
       "      <td>337</td>\n",
       "      <td>42.321081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.22</td>\n",
       "      <td>Fair</td>\n",
       "      <td>E</td>\n",
       "      <td>VS2</td>\n",
       "      <td>65.1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>337</td>\n",
       "      <td>36.425214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>VS1</td>\n",
       "      <td>59.4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>338</td>\n",
       "      <td>38.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>339</td>\n",
       "      <td>49.658700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>J</td>\n",
       "      <td>VS1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>340</td>\n",
       "      <td>37.704420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.22</td>\n",
       "      <td>Premium</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>342</td>\n",
       "      <td>34.715136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>344</td>\n",
       "      <td>51.515745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>60.2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>345</td>\n",
       "      <td>32.262375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.32</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>I1</td>\n",
       "      <td>60.9</td>\n",
       "      <td>58.0</td>\n",
       "      <td>345</td>\n",
       "      <td>51.883728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>I</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>348</td>\n",
       "      <td>50.130472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.4</td>\n",
       "      <td>54.0</td>\n",
       "      <td>351</td>\n",
       "      <td>48.996090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>351</td>\n",
       "      <td>48.833658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.7</td>\n",
       "      <td>59.0</td>\n",
       "      <td>351</td>\n",
       "      <td>47.818022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Good</td>\n",
       "      <td>I</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>56.0</td>\n",
       "      <td>351</td>\n",
       "      <td>49.641780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS2</td>\n",
       "      <td>63.8</td>\n",
       "      <td>55.0</td>\n",
       "      <td>352</td>\n",
       "      <td>37.428160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>VS1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>353</td>\n",
       "      <td>37.601784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.4</td>\n",
       "      <td>62.0</td>\n",
       "      <td>353</td>\n",
       "      <td>50.952974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI1</td>\n",
       "      <td>58.1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>353</td>\n",
       "      <td>51.403212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>G</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>60.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>354</td>\n",
       "      <td>38.366477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS1</td>\n",
       "      <td>62.5</td>\n",
       "      <td>57.0</td>\n",
       "      <td>355</td>\n",
       "      <td>38.635246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>J</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>357</td>\n",
       "      <td>49.138680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>VS2</td>\n",
       "      <td>60.5</td>\n",
       "      <td>61.0</td>\n",
       "      <td>357</td>\n",
       "      <td>37.730880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>60.9</td>\n",
       "      <td>57.0</td>\n",
       "      <td>357</td>\n",
       "      <td>38.236968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>402</td>\n",
       "      <td>38.849200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>402</td>\n",
       "      <td>39.693808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>60.7</td>\n",
       "      <td>59.0</td>\n",
       "      <td>402</td>\n",
       "      <td>38.525674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>59.5</td>\n",
       "      <td>58.0</td>\n",
       "      <td>402</td>\n",
       "      <td>39.073440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>VS1</td>\n",
       "      <td>61.9</td>\n",
       "      <td>58.0</td>\n",
       "      <td>402</td>\n",
       "      <td>37.876608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>58.2</td>\n",
       "      <td>59.0</td>\n",
       "      <td>402</td>\n",
       "      <td>39.258576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>64.1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>402</td>\n",
       "      <td>36.273930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>402</td>\n",
       "      <td>50.847225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0.26</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>VS2</td>\n",
       "      <td>60.8</td>\n",
       "      <td>59.0</td>\n",
       "      <td>403</td>\n",
       "      <td>43.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>0.33</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>I</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.8</td>\n",
       "      <td>55.0</td>\n",
       "      <td>403</td>\n",
       "      <td>56.294722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  carat        cut color clarity  depth  table  price     volume\n",
       "0            1   0.23      Ideal     E     SI2   61.5   55.0    326  38.202030\n",
       "1            2   0.21    Premium     E     SI1   59.8   61.0    326  34.505856\n",
       "2            3   0.23       Good     E     VS1   56.9   65.0    327  38.076885\n",
       "3            4   0.29    Premium     I     VS2   62.4   58.0    334  46.724580\n",
       "4            5   0.31       Good     J     SI2   63.3   58.0    335  51.917250\n",
       "5            6   0.24  Very Good     J    VVS2   62.8   57.0    336  38.693952\n",
       "6            7   0.24  Very Good     I    VVS1   62.3   57.0    336  38.830870\n",
       "7            8   0.26  Very Good     H     SI1   61.9   55.0    337  42.321081\n",
       "8            9   0.22       Fair     E     VS2   65.1   61.0    337  36.425214\n",
       "9           10   0.23  Very Good     H     VS1   59.4   61.0    338  38.718000\n",
       "10          11   0.30       Good     J     SI1   64.0   55.0    339  49.658700\n",
       "11          12   0.23      Ideal     J     VS1   62.8   56.0    340  37.704420\n",
       "12          13   0.22    Premium     F     SI1   60.4   61.0    342  34.715136\n",
       "13          14   0.31      Ideal     J     SI2   62.2   54.0    344  51.515745\n",
       "14          15   0.20    Premium     E     SI2   60.2   62.0    345  32.262375\n",
       "15          16   0.32    Premium     E      I1   60.9   58.0    345  51.883728\n",
       "16          17   0.30      Ideal     I     SI2   62.0   54.0    348  50.130472\n",
       "17          18   0.30       Good     J     SI1   63.4   54.0    351  48.996090\n",
       "18          19   0.30       Good     J     SI1   63.8   56.0    351  48.833658\n",
       "19          20   0.30  Very Good     J     SI1   62.7   59.0    351  47.818022\n",
       "20          21   0.30       Good     I     SI2   63.3   56.0    351  49.641780\n",
       "21          22   0.23  Very Good     E     VS2   63.8   55.0    352  37.428160\n",
       "22          23   0.23  Very Good     H     VS1   61.0   57.0    353  37.601784\n",
       "23          24   0.31  Very Good     J     SI1   59.4   62.0    353  50.952974\n",
       "24          25   0.31  Very Good     J     SI1   58.1   62.0    353  51.403212\n",
       "25          26   0.23  Very Good     G    VVS2   60.4   58.0    354  38.366477\n",
       "26          27   0.24    Premium     I     VS1   62.5   57.0    355  38.635246\n",
       "27          28   0.30  Very Good     J     VS2   62.2   57.0    357  49.138680\n",
       "28          29   0.23  Very Good     D     VS2   60.5   61.0    357  37.730880\n",
       "29          30   0.23  Very Good     F     VS1   60.9   57.0    357  38.236968\n",
       "30          31   0.23  Very Good     F     VS1   60.0   57.0    402  38.849200\n",
       "31          32   0.23  Very Good     F     VS1   59.8   57.0    402  39.693808\n",
       "32          33   0.23  Very Good     E     VS1   60.7   59.0    402  38.525674\n",
       "33          34   0.23  Very Good     E     VS1   59.5   58.0    402  39.073440\n",
       "34          35   0.23  Very Good     D     VS1   61.9   58.0    402  37.876608\n",
       "35          36   0.23       Good     F     VS1   58.2   59.0    402  39.258576\n",
       "36          37   0.23       Good     E     VS1   64.1   59.0    402  36.273930\n",
       "37          38   0.31       Good     H     SI1   64.0   54.0    402  50.847225\n",
       "38          39   0.26  Very Good     D     VS2   60.8   59.0    403  43.295616\n",
       "39          40   0.33      Ideal     I     SI2   61.8   55.0    403  56.294722"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_diam.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2daae3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03452882, 0.10332035, 0.42237385, 0.21475274, 0.22502425]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = res_log.model.predict(res_log.params, \n",
    "                                 exog=[[36.425214,337,0.22]])\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785892bc",
   "metadata": {},
   "source": [
    "'Fair', 'Good', 'Ideal', 'Very Good', 'Premium'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
